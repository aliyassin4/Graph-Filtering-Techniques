{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eca69f1",
   "metadata": {},
   "source": [
    "# Need to fix the h-index. why average weighted link devided by nodes same question for betwennes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae68854",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2837800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import community.community_louvain as community\n",
    "import os, sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306fc1e",
   "metadata": {},
   "source": [
    "# Extract Backbones from Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f86a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#import all utils modules\n",
    "from Utils.utils import shannon_entropy\n",
    "\n",
    "#--------------------------------------------\n",
    "#import all backbones modules\n",
    "from Backbones import h_backbone as hb\n",
    "from Backbones import disparity_filter as disf\n",
    "from Backbones.doubly_stochastic import read, doubly_stochastic as ds\n",
    "from Backbones.high_salience_skeleton import high_salience_skeleton as hss\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "#create a dictionary to save the backbones and their parameters used for each network\n",
    "backbones = []\n",
    "#network_backbones= dict()\n",
    "network_backbone_parameters = dict()\n",
    "network_backbone_measures = dict()\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "#read network names from the dataset folder\n",
    "networks = ['lesmis']\n",
    "\n",
    "\n",
    "#loop through each network and extract the backbones\n",
    "for network in networks:\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #read edge list from csv file\n",
    "    edge_list = pd.read_csv('../Datasets/' + network + '.csv')\n",
    "    \n",
    "    #read edge list from csv file for the doubly stochastic filter, noice corrected and the high salience skeleton\n",
    "    table, nnodes, nnedges = read(\"../Datasets/\" + network + '.csv', \"weight\", sep=',', consider_self_loops=False, triangular_input = True, undirected=True) \n",
    "\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #create graph from the edge list\n",
    "    G = nx.from_pandas_edgelist(edge_list, edge_attr='weight', create_using=nx.Graph())\n",
    "    \n",
    "    #extract the number of nodes N and number of edges E from the graph and the percentage of edges to preserce \n",
    "    N = len(G.nodes())\n",
    "    E = len(G.edges())\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #create numpy array having the top 30% values as True while others are False\n",
    "    E_percentage = int(0.3*E)\n",
    "    backbone_edges = np.full(E, False)\n",
    "    backbone_edges[0:E_percentage] = True\n",
    "    \n",
    "    #create the results dataframe and edge column that is used to merge backbones dataframes later\n",
    "    backbone_results = edge_list.copy()\n",
    "    backbone_results[\"edge\"] = backbone_results.apply(lambda x: \"%s-%s\" % (min(x[\"source\"], x[\"target\"]), max(x[\"source\"], x[\"target\"])), axis = 1)\n",
    "    \n",
    "    #initialize the network dictionary to save the backbones parameters used\n",
    "    network_backbone_parameters[network] = dict()\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #append the backbone short name to the list of backbones\n",
    "    backbones.append('df')\n",
    "    \n",
    "    #apply the disparity filter algorithm\n",
    "    backbone = disf.disparity_filter(G)\n",
    "    \n",
    "    #create an edge list from the result graph with the scores\n",
    "    df_backbone = nx.to_pandas_edgelist(backbone)\n",
    "    \n",
    "    #sort alpha values of the edges\n",
    "    df_backbone = df_backbone.sort_values(by='alpha')\n",
    "    \n",
    "    #add a new column that assigns a boolean values for each edge stating if it is included in the backbone or not\n",
    "    df_backbone = df_backbone.assign(df_backbone = backbone_edges)\n",
    "    \n",
    "    #extract the alpha-value that gives this percentage of edges and add it to the backbones parameters dictionary\n",
    "    df_alpha = round(df_backbone[E_percentage-1:E_percentage]['alpha'].iloc[0],3)\n",
    "    network_backbone_parameters[network]['df_alpha'] = df_alpha\n",
    "    \n",
    "    #create edge column that is used to merge backbone with the backbone_results dataframe\n",
    "    df_backbone[\"edge\"] = df_backbone.apply(lambda x: \"%s-%s\" % (min(x[\"source\"], x[\"target\"]), max(x[\"source\"], x[\"target\"])), axis = 1)\n",
    "    \n",
    "    #drop the weights column from the backbone dataframe(already available in the main dataframe) and rename the columns\n",
    "    df_backbone = df_backbone.drop(columns=['source', 'target', 'weight'])\n",
    "    df_backbone.columns=['df_alpha', 'df_backbone', 'edge']\n",
    "\n",
    "    #merge the disparity filter results to the backbone_results results dataframe\n",
    "    backbone_results = pd.merge(backbone_results,df_backbone, on=['edge'])\n",
    "\n",
    "\n",
    "    #--------------------------------------------\n",
    "    #append the backbone short name to the list of backbones\n",
    "    backbones.append('ds')\n",
    "    \n",
    "    #apply the doubly stochastic filter algorithm\n",
    "    ds_backbone = ds(table, undirected = True, return_self_loops = False)\n",
    "\n",
    "    #sort score values of the edges\n",
    "    ds_backbone = ds_backbone.sort_values(by='score', ascending=False)\n",
    "    \n",
    "    #add a new column that assigns a boolean values for each edge stating if it is included in the backbone or not\n",
    "    ds_backbone = ds_backbone.assign(ds_backbone = backbone_edges)\n",
    "        \n",
    "    #create edge column that is used to merge backbone with the backbone_results dataframe\n",
    "    ds_backbone[\"edge\"] = ds_backbone.apply(lambda x: \"%s-%s\" % (min(x[\"source\"], x[\"target\"]), max(x[\"source\"], x[\"target\"])), axis = 1)\n",
    "    \n",
    "    #drop the weights column from the backbone dataframe(already available in the main dataframe) and rename the columns\n",
    "    ds_backbone = ds_backbone.drop(columns=['source', 'target', 'weight'])\n",
    "    ds_backbone.columns=['ds_score', 'ds_backbone', 'edge']\n",
    "    \n",
    "    #merge the doubly stochastic filter results to the backbone_results results dataframe\n",
    "    backbone_results = pd.merge(backbone_results,ds_backbone, on=['edge'])\n",
    "\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #append the backbone short name to the list of backbones\n",
    "    backbones.append('hss')\n",
    "    \n",
    "    #apply the high salience skeleton algorithm\n",
    "    hss_backbone = hss(table, return_self_loops=False, undirected=True)\n",
    "    \n",
    "    #sort score values of the edges\n",
    "    hss_backbone = hss_backbone.sort_values(by='score', ascending=False)\n",
    "    \n",
    "    #add a new column that assigns a boolean values for each edge stating if it is included in the backbone or not\n",
    "    hss_backbone = hss_backbone.assign(hss_backbone = backbone_edges)\n",
    "    \n",
    "    #extract the score-value that gives this percentage of edges and add the value to the dictionary\n",
    "    hss_score = round(hss_backbone[E_percentage-1:E_percentage]['score'].iloc[0],3)\n",
    "    network_backbone_parameters[network]['hss_score'] = hss_score\n",
    "    \n",
    "    #create edge column that is used to merge backbone with the backbone_results dataframe\n",
    "    hss_backbone[\"edge\"] = hss_backbone.apply(lambda x: \"%s-%s\" % (min(x[\"source\"], x[\"target\"]), max(x[\"source\"], x[\"target\"])), axis = 1)\n",
    "    \n",
    "    #drop the weights column from the backbone dataframe(already available in the main dataframe) and rename the columns\n",
    "    hss_backbone = hss_backbone.drop(columns=['source', 'target', 'weight'])\n",
    "    hss_backbone.columns=['hss_score', 'hss_backbone', 'edge']\n",
    "    \n",
    "    #merge the high salience skeleton results to the backbone_results results dataframe\n",
    "    backbone_results = pd.merge(backbone_results,hss_backbone, on=['edge'])\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #append the backbone short name to the list of backbones\n",
    "    backbones.append('h')\n",
    "    \n",
    "    #apply the disparity filter algorithm and add the h-bridge and h-weight values to the dictionary\n",
    "    h_bridge, h_weight, backbone = hb.h_backbone(G)\n",
    "    network_backbone_parameters[network]['h_bridge'] = h_bridge\n",
    "    network_backbone_parameters[network]['h_weight'] = h_weight\n",
    "    \n",
    "    #create an edge list from the result graph with the scores\n",
    "    h_backbone = nx.to_pandas_edgelist(backbone)\n",
    "    \n",
    "    #create edge column that is used to merge backbone with the backbone_results dataframe\n",
    "    h_backbone[\"edge\"] = h_backbone.apply(lambda x: \"%s-%s\" % (min(x[\"source\"], x[\"target\"]), max(x[\"source\"], x[\"target\"])), axis = 1)\n",
    "    \n",
    "    #drop the weights column from the backbone dataframe(already available in the main dataframe) and rename the columns\n",
    "    h_backbone = h_backbone.drop(columns=['source', 'target', 'weight'])\n",
    "    #h_backbone.columns=['h_backbone', 'h_bridge', 'edge']\n",
    "    \n",
    "    #merge the h_backbone results to the backbone_results results dataframe\n",
    "    backbone_results = pd.merge(backbone_results,h_backbone, on=['edge'])\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #drop the edge column before saving \n",
    "    backbone_results = backbone_results.drop(columns='edge')\n",
    "    \n",
    "    #save backbone results to csv file and to a dictionary for further calculations\n",
    "    #network_backbones[network] = backbone_results\n",
    "    backbone_results.to_csv('../Results/Backbones Results/' + network + '.csv', index=False)\n",
    "    \n",
    "    #save backbone parameters to csv file and to a dictionary for further calculations\n",
    "    network_backbones_parameters = pd.DataFrame({network: network_backbone_parameters[network]})\n",
    "    network_backbones_parameters.to_csv('../Results/Backbones Results/' + network + '_params.csv', index=False)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #calculate evaluation measures: the structural properties\n",
    "    measures = ['nodes_fraction', 'average_weighted_degree', 'average_link_weight', 'average_betweeness', 'density', 'entropy', 'weighted_modularity']\n",
    "    \n",
    "    #initialize the dataframe measures for the network backbones\n",
    "    network_measures = pd.DataFrame(columns = [name+'_backbone' for name in backbones], index=measures)\n",
    "        \n",
    "    #loop through all extracted backbones for the network\n",
    "    for backbone_name in backbones:\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #create the backbone graph after extracting only edges are preserved in the backbone.i.e only True values of _backbone \n",
    "        backbone = backbone_results[backbone_results[backbone_name + '_backbone']][['source', 'target', 'weight']]\n",
    "        G = nx.from_pandas_edgelist(backbone, edge_attr='weight', create_using=nx.Graph())\n",
    "        \n",
    "        #take only the largest connected component\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G = G.subgraph(largest_cc).copy()\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the fraction of nodes preserved\n",
    "        node_fraction = len(G.nodes())/N\n",
    "        network_measures[backbone_name+'_backbone']['nodes_fraction'] = node_fraction\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the average weighted degree \n",
    "        average_weighted_degree = sum([G.degree(node, weight='weight') for node in G.nodes()])/len(G.nodes())\n",
    "        network_measures[backbone_name+'_backbone']['average_weighted_degree'] = average_weighted_degree\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the average link weight \n",
    "        average_link_weight = sum([G.edges()[edge]['weight'] for edge in G.edges()])/len(G.nodes())\n",
    "        network_measures[backbone_name+'_backbone']['average_link_weight'] = average_link_weight\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the average betwenness\n",
    "        average_betweeness = sum(nx.edge_betweenness_centrality(G, weight='weight', normalized=False).values())/len(G.nodes())\n",
    "        network_measures[backbone_name+'_backbone']['average_betweeness'] = average_betweeness\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the density\n",
    "        density = round((2*len(G.edges()))/(len(G.nodes())*(len(G.nodes())-1)), 3)\n",
    "        network_measures[backbone_name+'_backbone']['density'] = density\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the entropy\n",
    "        entropy = round(shannon_entropy(G), 3)\n",
    "        network_measures[backbone_name+'_backbone']['entropy'] = entropy\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------\n",
    "        #calculate the weighted modulartiy\n",
    "        communities = community.best_partition(G, random_state=1)\n",
    "        weighted_modularity = round(community.modularity(communities, G, weight='weight'), 3)\n",
    "        network_measures[backbone_name+'_backbone']['weighted_modularity'] = weighted_modularity\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #add the measurese of all backbones to the dictionary of measures for all network and save it to csv\n",
    "    network_backbone_measures[network] = network_measures\n",
    "    network_measures.to_csv('../Results/Backbones Results/' + network + '_measures.csv', index=False)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
